name: Reqs2tests
run-name: ${{ github.ref_name }} is being tested
on:
  workflow_dispatch:
    inputs:
      env-set:
        description: 'Select the environment set to run the tests (`sanity`, `piinnovo`, `atg-customer`)'
        required: false
        default: 'sanity'
      max-cost:
        description: 'Maximum cost of the tests (positive float, 0 for unlimited)'
        required: false
        default: '0'
jobs:
  reqs2tests_eval:
    permissions: write-all
    runs-on: [self-hosted, reqs2tests]
    env:
      VECTORCAST_DIR: /vcast/${{ vars.VCAST_VERSION }}
      VECTOR_LICENSE_FILE: /vcast/vector-license.lic
      ENV_SET_NAME: ${{ github.event.inputs.env-set }}
      MAX_COST: ${{ github.event.inputs.max-cost }}
      PIINNOVO_SRC: "https://rds-vtc-docker-dev-local.vegistry.vg.vector.int/artifactory/rds-build-packages-generic-dev-local/code2reqs2tests/piinnovo-source.tar.gz"
      HALLA_SRC: "https://rds-vtc-docker-dev-local.vegistry.vg.vector.int/artifactory/rds-build-packages-generic-dev-local/code2reqs2tests/halla-modmgr4a-source.tar.gz"
      OPENAI_API_BASE: ${{ vars.OPENAI_API_BASE }}
      OPENAI_API_KEY: ${{ secrets.OPENAI_API_KEY }}
      OPENAI_GENERATION_DEPLOYMENT: ${{ vars.OPENAI_GENERATION_DEPLOYMENT }}
      OPENAI_ADVANCED_GENERATION_DEPLOYMENT: ${{ vars.OPENAI_ADVANCED_GENERATION_DEPLOYMENT }}
      OPENAI_EMBEDDING_DEPLOYMENT: ${{ vars.OPENAI_EMBEDDING_DEPLOYMENT }}
      AUTOREQ_MLFLOW_SERVER: ${{ vars.AUTOREQ_MLFLOW_SERVER }}
      VCAST_USER_HOME: /home/vcast_user

    container:
      image: rds-vtc-docker-dev-local.vegistry.vg.vector.int/vcast/reqs2tests_ci:latest
      options: --user vcast_user --mount type=bind,source=${{ vars.VCAST_RELEASES_PATH }},target=/vcast

    steps:
      - name: Check out repository
        id: checkout
        uses: actions/checkout@v4
        continue-on-error: true

      - name: Wait before retry
        id: should-retry-checkout
        if: failure()
        continue-on-error: true
        run: |
          sleep 10
          exit 1

      - name: Check out repository (retry)
        if: failure()
        uses: actions/checkout@v4

      - name: Virtual environment restore from cache
        uses: actions/cache/restore@v4
        id: cache-venv-restore
        with:
          path: ${{ env.VCAST_USER_HOME }}/.venv/
          key: ${{ runner.os }}-venv-${{ hashFiles('./setup.py') }}

      - name: Setup Python virtual environment
        if: steps.cache-venv-restore.outputs.cache-hit != 'true'
        run: |
          python3.10 -m venv ${{ env.VCAST_USER_HOME }}/.venv
          source ${{ env.VCAST_USER_HOME }}/.venv/bin/activate
          pip install --no-cache --upgrade pip
          pip install --no-cache --editable .
          deactivate
        shell: bash

      - name: Virtual environment save cache
        if: steps.cache-venv-restore.outputs.cache-hit != 'true'
        uses: actions/cache/save@v4
        with:
          path: ${{ env.VCAST_USER_HOME }}/.venv/
          key: ${{ runner.os }}-venv-${{ hashFiles('./setup.py') }}

      - name: Reqs2tests evaluation
        run: |
          ./ci/reqs2tests_eval.sh
        shell: bash

      - name: Generate reports
        run: |
          source $VCAST_USER_HOME/.venv/bin/activate
          python evaluation/create_report.py --input ${{ env.VCAST_USER_HOME }}/.envs/r2t_eval_results --output ${{ env.VCAST_USER_HOME }}/.envs/report.html
          markdownify ${{ env.VCAST_USER_HOME }}/.envs/report.html > ${{ env.VCAST_USER_HOME }}/.envs/results.md
          deactivate
        shell: bash

      - name: Upload reports
        if: always()
        run: |
          COMMIT_DATE=$(git log -1 --format=%cd --date=format:%Y-%m-%dT%H:%M:%S)
          BASE_ARTIFACTORY_URL="https://artifactory.vi.vector.int:443/artifactory/rds-build-packages-generic-dev/code2reqs2tests/tests-results/${{ github.ref_name }}/$COMMIT_DATE-${{ github.sha }}/${{ env.ENV_SET_NAME }}"
          REPORT_URL=$BASE_ARTIFACTORY_URL/report.html
          RESULTS_URL=$BASE_ARTIFACTORY_URL/r2t_eval_results/
          curl -H "X-JFrog-Art-Api:${{ secrets.ARTIFACTORY_TOKEN }}" -X PUT $REPORT_URL -T ${{ env.VCAST_USER_HOME }}/.envs/report.html
          cd ${{ env.VCAST_USER_HOME }}/.envs/r2t_eval_results
          tar -cvzf ../to_upload.tar.gz . > /dev/null
          cd ..
          curl -H "X-Explode-Archive: true" -H "X-JFrog-Art-Api:${{ secrets.ARTIFACTORY_TOKEN }}" -X PUT $RESULTS_URL -T ./to_upload.tar.gz
          
          echo '## HTML Report' >> $GITHUB_STEP_SUMMARY
          echo "[See report]($REPORT_URL)" >> $GITHUB_STEP_SUMMARY
          echo "## Results folder" >> $GITHUB_STEP_SUMMARY
          echo "[Results folder]($RESULTS_URL)" >> $GITHUB_STEP_SUMMARY
          
          echo "## Markdown results" >> $GITHUB_STEP_SUMMARY
          cat ${{ env.VCAST_USER_HOME }}/.envs/results.md >> $GITHUB_STEP_SUMMARY
        shell: bash

      - name: Run evaluation
        if: always()
        run: |
          python3.10 ./ci/run_evaluation.py ${{ env.VCAST_USER_HOME }}/.envs/r2t_eval_results
          echo '## Run checks' >> $GITHUB_STEP_SUMMARY
          echo '```' >> $GITHUB_STEP_SUMMARY
          if [[ -f requirements_check_errors.txt ]] ; then
            echo "Errors found in requirements_check_errors.txt"
            cat requirements_check_errors.txt >> $GITHUB_STEP_SUMMARY
            echo '```' >> $GITHUB_STEP_SUMMARY
            exit 1
          fi
          echo 'No errors found' >> $GITHUB_STEP_SUMMARY
          echo '```' >> $GITHUB_STEP_SUMMARY
        shell: bash
